- title: Multimodal Attribute Extraction
  image: assets/projects/
  authors:
      - name: Robert L. Logan IV
        website: https://rloganiv.github.io
      - name: Samuel Humeau
      - name: Sameer Singh
        website: http://www.sameersingh.org
  venue: 6th Workshop on Automated Knowledge Base Construction (AKBC) 2017
  type: workshop
  abstract: >
      The broad goal of information extraction is to derive structured
      information from unstructured data. However, most existing methods focus
      solely on text, ignoring other types of unstructured data such as images,
      video and audio which comprise an increasing portion of the information
      on the web. To address this shortcoming, we propose the task of
      multimodal attribute extraction. Given a collection of unstructured and
      semi-structured contextual information about an entity (such as a textual
      description, or visual depictions) the task is to extract the entity's
      underlying attributes. In this paper, we provide a dataset containing
      mixed-media data for over 2 million product items along with 7 million
      attribute-value pairs describing the items which can be used to train
      attribute extractors in a weakly supervised manner. We provide a variety
      of baselines which demonstrate the relative effectiveness of the
      individual modes of information towards solving the task, as well as
      study human performance.
  arxiv: https://arxiv.org/abs/1711.11118
  poster: assets/projects/mae-poster.pdf
  github: https://github.com/rloganiv/mae-model
  dataset: https://rloganiv.github.io/mae

